{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Transcription Notebook\n",
    "\n",
    "This notebook can be used to transcribe audio files. The transcriptions can be exported as a text file or\n",
    " as a csv (tsv) file.\n",
    "\n",
    "\n",
    "Libraries that need to be installed:\n",
    "- [Whisper](https://github.com/openai/whisper)\n",
    "\n",
    "Libraries Dependencies:\n",
    "- [ffmpeg](https://ffmpeg.org/)\n",
    "\n",
    "Detailed step-by-step instructions for installing the required libraries can be found in the\n",
    " [README.md](https://github.com/LeonMunz/OAI_Whisper#readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import os\n",
    "import whisper\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1️⃣ Path details\n",
    "In this section, the path and the file name can be specified. The default path is \"audio/...\"."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Filename\n",
    "file_name = 'Aufnahme_Gruppe12.m4a'\n",
    "\n",
    "# Path to audio file\n",
    "audio = os.path.join('audio', file_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2️⃣ Select model:\n",
    "The model to be used by whisper can be selected here. The results of the individual models may differ from each other.\n",
    "A simple overview of the results of the different models can be found in the\n",
    "[README.md](https://github.com/LeonMunz/OAI_Whisper#readme).\n",
    "\n",
    "\n",
    "There are five model sizes, four with English-only versions, offering speed and accuracy tradeoffs. Below are the names of the available models and their approximate memory requirements and relative speed.\n",
    "\n",
    "\n",
    "|  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n",
    "|:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n",
    "|  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~32x      |\n",
    "|  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~16x      |\n",
    "| small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~6x       |\n",
    "| medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |\n",
    "| large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Choose model size\n",
    "model_size = 'medium'\n",
    "\n",
    "# Load model\n",
    "model = whisper.load_model(model_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3️⃣ Read audio file:\n",
    "\n",
    "If this warning appears during execution, it can be ignored.\n",
    "\n",
    "```UserWarning: FP16 is not supported on CPU; using FP32 instead w\n",
    "arnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pycharmproject\\oai_whisper\\whisper_venv\\lib\\site-packages\\whisper\\transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "# Read the entire file and processes the audio\n",
    "res = model.transcribe(audio)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4️⃣ Generate Output:\n",
    "First, a data frame is created to check the transcription within the notebook. In this DataFrame, the start and end\n",
    "time of each extracted segment is given in seconds, as well as the extracted text.\n",
    "\n",
    "Secondly, the output format can be selected, with the choice between a .txt file or a .csv (tsv) file."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "     start    end  \\\n0      0.0    3.0   \n1      3.0    5.0   \n2      5.0    6.0   \n3      6.0   12.0   \n4     12.0   16.0   \n..     ...    ...   \n119  674.0  677.0   \n120  677.0  680.0   \n121  680.0  681.0   \n122  681.0  683.0   \n123  683.0  686.0   \n\n                                                                                           text  \n0                                                                                    Steinfeld?  \n1                                                       Hallo, hier ist Tim Wittich, guten Tag.  \n2                                                                                    Guten Tag.  \n3     Ich bin jetzt mit meinem Kumpel hier und wir würden gerne das Interview mit Ihnen führen.  \n4                                       Ja, seid ihr schon an der Türe oder seid ihr unterwegs?  \n..                                                                                          ...  \n119                                                           Ja, super, können wir machen, ja.  \n120                               Ja, dann bedanke ich mich und wünsche Ihnen auf jeden Fall...  \n121                                    Ja, danke, ich wünsche Ihnen auf jeden Fall viel Erfolg.  \n122                                    Ich wünsche Ihnen auf jeden Fall ein schönes Wochenende.  \n123                                                                Auch so, jo, danke, tschüss.  \n\n[124 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>start</th>\n      <th>end</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>Steinfeld?</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>Hallo, hier ist Tim Wittich, guten Tag.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5.0</td>\n      <td>6.0</td>\n      <td>Guten Tag.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6.0</td>\n      <td>12.0</td>\n      <td>Ich bin jetzt mit meinem Kumpel hier und wir würden gerne das Interview mit Ihnen führen.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>12.0</td>\n      <td>16.0</td>\n      <td>Ja, seid ihr schon an der Türe oder seid ihr unterwegs?</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>119</th>\n      <td>674.0</td>\n      <td>677.0</td>\n      <td>Ja, super, können wir machen, ja.</td>\n    </tr>\n    <tr>\n      <th>120</th>\n      <td>677.0</td>\n      <td>680.0</td>\n      <td>Ja, dann bedanke ich mich und wünsche Ihnen auf jeden Fall...</td>\n    </tr>\n    <tr>\n      <th>121</th>\n      <td>680.0</td>\n      <td>681.0</td>\n      <td>Ja, danke, ich wünsche Ihnen auf jeden Fall viel Erfolg.</td>\n    </tr>\n    <tr>\n      <th>122</th>\n      <td>681.0</td>\n      <td>683.0</td>\n      <td>Ich wünsche Ihnen auf jeden Fall ein schönes Wochenende.</td>\n    </tr>\n    <tr>\n      <th>123</th>\n      <td>683.0</td>\n      <td>686.0</td>\n      <td>Auch so, jo, danke, tschüss.</td>\n    </tr>\n  </tbody>\n</table>\n<p>124 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a DataFrame from transcription data\n",
    "df = pd.DataFrame(res['segments'])\n",
    "df = df.drop(['id', 'seek', 'tokens', 'temperature', 'avg_logprob', 'compression_ratio', 'no_speech_prob'], axis=1)\n",
    "pd.set_option('max_colwidth', None)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file \"test_output\" was created in the format \"txt\".\n"
     ]
    }
   ],
   "source": [
    "def output(filename, path, out_type):\n",
    "    if out_type == 'csv':\n",
    "        df.to_csv(path + filename + '.csv', sep=';')\n",
    "    elif out_type == 'txt':\n",
    "        with open(path + '.txt', 'w') as f:\n",
    "            for x in df['text']:\n",
    "                f.write(x.strip() + '\\n')\n",
    "    return print('Output file \"{}\" was created in the format \"{}\".'.format(filename, out_type))\n",
    "\n",
    "# Output filename\n",
    "out_file_name = file_name\n",
    "# Output filepath\n",
    "output_path = os.path.join('text_output', out_file_name)\n",
    "# Output type (csv, txt)\n",
    "output_type = 'txt'\n",
    "\n",
    "output(out_file_name, output_path, output_type)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}