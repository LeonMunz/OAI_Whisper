{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Transcription Notebook\n",
    "\n",
    "This notebook can be used to transcribe audio files. The transcriptions can be exported as a text file or\n",
    " as a csv (tsv) file.\n",
    "\n",
    "\n",
    "Libraries that need to be installed:\n",
    "- [Whisper](https://github.com/openai/whisper)\n",
    "\n",
    "Libraries Dependencies:\n",
    "- [ffmpeg](https://ffmpeg.org/)\n",
    "\n",
    "Detailed step-by-step instructions for installing the required libraries can be found in the\n",
    " [README.md](https://github.com/LeonMunz/OAI_Whisper#readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "import os\n",
    "import whisper\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1️⃣ Path details\n",
    "In this section, the path and the file name can be specified. The default path is \"audio/...\"."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# Filename\n",
    "file_name = 'test_audio.wav'\n",
    "\n",
    "# Path to audio file\n",
    "audio = os.path.join('audio', file_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2️⃣ Select model:\n",
    "The model to be used by whisper can be selected here. The results of the individual models may differ from each other.\n",
    "A simple overview of the results of the different models can be found in the\n",
    "[README.md](https://github.com/LeonMunz/OAI_Whisper#readme).\n",
    "\n",
    "\n",
    "There are five model sizes, four with English-only versions, offering speed and accuracy tradeoffs. Below are the names of the available models and their approximate memory requirements and relative speed.\n",
    "\n",
    "\n",
    "|  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n",
    "|:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n",
    "|  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~32x      |\n",
    "|  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~16x      |\n",
    "| small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~6x       |\n",
    "| medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |\n",
    "| large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# Choose model size\n",
    "model_size = 'medium'\n",
    "\n",
    "# Load model\n",
    "model = whisper.load_model(model_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3️⃣ Read audio file:\n",
    "\n",
    "If this warning appears during execution, it can be ignored.\n",
    "\n",
    "```UserWarning: FP16 is not supported on CPU; using FP32 instead w\n",
    "arnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\pycharmproject\\oai_whisper\\whisper_venv\\lib\\site-packages\\whisper\\transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "# Read the entire file and processes the audio\n",
    "res = model.transcribe(audio)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4️⃣ Generate Output:\n",
    "First, a data frame is created to check the transcription within the notebook. In this DataFrame, the start and end\n",
    "time of each extracted segment is given in seconds, as well as the extracted text.\n",
    "\n",
    "Secondly, the output format can be selected, with the choice between a .txt file or a .csv (tsv) file."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "   start   end  \\\n0    0.0   5.0   \n1    5.0   8.0   \n2    8.0  11.0   \n3   11.0  13.0   \n4   13.0  17.0   \n5   17.0  21.0   \n6   21.0  24.0   \n7   24.0  27.0   \n8   27.0  30.0   \n\n                                                                              text  \n0   Beim Nachrichtendienst Twitter dachte sich wohl so mancher bei dem Pieps wohl.  \n1                           Seitdem Milliardär Elon Musk den Laden übernommen hat,  \n2                                    und gleich einmal Massenentlassungen vornahm.  \n3                                                  Der Letzte macht das Licht aus,  \n4                      mochte einem auch angesichts dieser Radikal-Cure einfallen.  \n5                       Bei Musk eine konstante Strategie zu erkennen, ist schwer.  \n6                      Klar ist wohl nur, er will den Dienst kommerzieller machen.  \n7                           Vergaulte aber durch seine Aktionen schon Werbekunden.  \n8                                       Und er will weniger inhaltlich eingreifen.  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>start</th>\n      <th>end</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>Beim Nachrichtendienst Twitter dachte sich wohl so mancher bei dem Pieps wohl.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5.0</td>\n      <td>8.0</td>\n      <td>Seitdem Milliardär Elon Musk den Laden übernommen hat,</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.0</td>\n      <td>11.0</td>\n      <td>und gleich einmal Massenentlassungen vornahm.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.0</td>\n      <td>13.0</td>\n      <td>Der Letzte macht das Licht aus,</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13.0</td>\n      <td>17.0</td>\n      <td>mochte einem auch angesichts dieser Radikal-Cure einfallen.</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>17.0</td>\n      <td>21.0</td>\n      <td>Bei Musk eine konstante Strategie zu erkennen, ist schwer.</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>21.0</td>\n      <td>24.0</td>\n      <td>Klar ist wohl nur, er will den Dienst kommerzieller machen.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>24.0</td>\n      <td>27.0</td>\n      <td>Vergaulte aber durch seine Aktionen schon Werbekunden.</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>27.0</td>\n      <td>30.0</td>\n      <td>Und er will weniger inhaltlich eingreifen.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a DataFrame from transcription data\n",
    "df = pd.DataFrame(res['segments'])\n",
    "df = df.drop(['id', 'seek', 'tokens', 'temperature', 'avg_logprob', 'compression_ratio', 'no_speech_prob'], axis=1)\n",
    "pd.set_option('max_colwidth', None)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file \"test_output\" was created in the format \"txt\"\n"
     ]
    }
   ],
   "source": [
    "def output(filename, path, out_type):\n",
    "    if out_type == 'csv':\n",
    "        df.to_csv(path + filename + '.csv', sep=';')\n",
    "    elif out_type == 'txt':\n",
    "        with open(path + '.txt', 'a') as f:\n",
    "            df_as_string = df.to_string(header=False, index=False)\n",
    "            f.write(df_as_string)\n",
    "    return print('Output file \"{}\" was created in the format \"{}\".'.format(filename, out_type))\n",
    "\n",
    "# Output filename\n",
    "out_file_name = 'test_output'\n",
    "# Output filepath\n",
    "output_path = os.path.join('text_output', out_file_name)\n",
    "# Output type (csv, txt)\n",
    "output_type = 'txt'\n",
    "\n",
    "output(out_file_name, output_path, output_type)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}